<div align="center">
  <h1>DIVE into MoE</h1>
  <div>
    <a href="#overview">ğŸ“ Overview</a> | <a href="#installation">âš™ï¸ Installation Guide</a> | <a href="#quick-start">ğŸš€ Quick Start</a> | <a href="#method">ğŸš… Method Details</a> | <a href="#evaluation">ğŸ’ Evaluation</a>
  </div>
</div>


<h2 id="todo">ğŸ“¦ To be released</h2>

- [ ] script usage

<h2 id="overview">ğŸ“ Overview</h2>

This repository contains the official implementation of our ACL 2025 paper "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts".

<h2 id="installation">âš™ï¸ Installation</h2>

Step 1: Create a new conda environment:
```
conda create -n dive python=3.9
conda activate dive
```
Step 2: Install relevant packages
```
conda install pytorch==2.7.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=11.3 -c pytorch -c conda-forge
pip install -r requirements.txt
```

<h2 id="quick-start">ğŸš€ Quick Start</h2>

Put training datasets in the `datasets` folder.

<h2 id="method">ğŸš… Method Details</h2>

<h2 id="evaluation">ğŸ’ Evaluation</h2>

<h2 id="citation">ğŸ’¬ Citation</h2>