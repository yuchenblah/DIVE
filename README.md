<div align="center">
  <h1>DIVE into MoE</h1>
  <div>
    <a href="#overview">📝 Overview</a> | <a href="#installation">⚙️ Installation Guide</a> | <a href="#quick-start">🚀 Quick Start</a> | <a href="#method">🚅 Method Details</a> | <a href="#evaluation">💎 Evaluation</a>
  </div>
</div>


<h2 id="todo">📦 To be released</h2>

- [ ] script usage

<h2 id="overview">📝 Overview</h2>

This repository contains the official implementation of our ACL 2025 paper "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts".

<h2 id="installation">⚙️ Installation</h2>

Step 1: Create a new conda environment:
```
conda create -n dive python=3.9
conda activate dive
```
Step 2: Install relevant packages
```
conda install pytorch==2.7.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=11.3 -c pytorch -c conda-forge
pip install -r requirements.txt
```

<h2 id="quick-start">🚀 Quick Start</h2>

Put training datasets in the `datasets` folder.

<h2 id="method">🚅 Method Details</h2>

<h2 id="evaluation">💎 Evaluation</h2>

<h2 id="citation">💬 Citation</h2>